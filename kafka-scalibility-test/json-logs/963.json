[
    {
        "timestamp": "2015-03-23 11:45:24,087",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.metrics2.impl.MetricsConfig",
        "message": " loaded properties from hadoop-metrics2.properties\n"
    },
    {
        "timestamp": "2015-03-23 11:45:24,620",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl",
        "message": " Scheduled snapshot period at 10 second(s).\n"
    },
    {
        "timestamp": "2015-03-23 11:45:24,620",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl",
        "message": " MapTask metrics system started\n"
    },
    {
        "timestamp": "2015-03-23 11:45:24,762",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.YarnChild",
        "message": " Executing with tokens:\n"
    },
    {
        "timestamp": "2015-03-23 11:45:24,762",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.YarnChild",
        "message": " Kind: mapreduce.job, Service: job_1427088391284_0095, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@a08feeb)\n"
    },
    {
        "timestamp": "2015-03-23 11:45:25,711",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.YarnChild",
        "message": " Sleeping for 0ms before retrying again. Got null now.\n"
    },
    {
        "timestamp": "2015-03-23 11:45:29,273",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.YarnChild",
        "message": " mapreduce.cluster.local.dir for child: /tmp/hadoop-ubuntu/nm-local-dir/usercache/ubuntu/appcache/application_1427088391284_0095\n"
    },
    {
        "timestamp": "2015-03-23 11:45:38,045",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.conf.Configuration.deprecation",
        "message": " session.id is deprecated. Instead, use dfs.metrics.session-id\n"
    },
    {
        "timestamp": "2015-03-23 11:45:41,405",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.Task",
        "message": "  Using ResourceCalculatorProcessTree : [ ]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:42,149",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.MapTask",
        "message": " Processing split: Paths:/HiBench/Hive/Input-comp/uservisits/part-00010:0+48471444InputFormatClass: org.apache.hadoop.mapred.SequenceFileInputFormat\n"
    },
    {
        "timestamp": "2015-03-23 11:45:42,412",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.hive.ql.log.PerfLogger",
        "message": " <PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>\n"
    },
    {
        "timestamp": "2015-03-23 11:45:42,412",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.hive.ql.exec.Utilities",
        "message": " Deserializing MapWork via kryo\n"
    },
    {
        "timestamp": "2015-03-23 11:45:43,706",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.hive.ql.log.PerfLogger",
        "message": " </PERFLOG method=deserializePlan start=1427111142412 end=1427111143706 duration=1294 from=org.apache.hadoop.hive.ql.exec.Utilities>\n"
    },
    {
        "timestamp": "2015-03-23 11:45:43,861",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.zlib.ZlibFactory",
        "message": " Successfully loaded & initialized native-zlib library\n"
    },
    {
        "timestamp": "2015-03-23 11:45:43,862",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:43,903",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:43,904",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:43,904",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,099",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,099",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,100",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,101",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.io.compress.CodecPool",
        "message": " Got brand-new decompressor [.deflate]\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,108",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader",
        "message": " Processing file hdfs://172.31.17.135:8120/HiBench/Hive/Input-comp/uservisits/part-00010\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,109",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.conf.Configuration.deprecation",
        "message": " map.input.file is deprecated. Instead, use mapreduce.map.input.file\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,109",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.conf.Configuration.deprecation",
        "message": " map.input.start is deprecated. Instead, use mapreduce.map.input.start\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,110",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.conf.Configuration.deprecation",
        "message": " map.input.length is deprecated. Instead, use mapreduce.map.input.length\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,110",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.MapTask",
        "message": " numReduceTasks: 96\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,675",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.MapTask",
        "message": " (EQUATOR) 0 kvi 26214396(104857584)\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,675",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.MapTask",
        "message": " mapreduce.task.io.sort.mb: 100\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,675",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.MapTask",
        "message": " soft limit at 83886080\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,675",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.MapTask",
        "message": " bufstart = 0; bufvoid = 104857600\n"
    },
    {
        "timestamp": "2015-03-23 11:45:44,675",
        "type": "INFO",
        "app": "[main] org.apache.hadoop.mapred.MapTask",
        "message": " kvstart = 26214396; length = 6553600\n"
    }
]